{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11903069,"sourceType":"datasetVersion","datasetId":7482430}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import OS module\nimport os\n\n# Imports\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import RandAugment\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:14:27.105600Z","iopub.execute_input":"2025-05-22T10:14:27.105933Z","iopub.status.idle":"2025-05-22T10:14:27.110859Z","shell.execute_reply.started":"2025-05-22T10:14:27.105911Z","shell.execute_reply":"2025-05-22T10:14:27.110074Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset_root = \"/kaggle/input/visual-plant-disease-detection/dataset\"\n\ntrain_dir = os.path.join(dataset_root,\"train\")\ntest_dir = os.path.join(dataset_root,\"test\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:14:28.474540Z","iopub.execute_input":"2025-05-22T10:14:28.474846Z","iopub.status.idle":"2025-05-22T10:14:28.478836Z","shell.execute_reply.started":"2025-05-22T10:14:28.474823Z","shell.execute_reply":"2025-05-22T10:14:28.478185Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Trainning with pure train dataset","metadata":{}},{"cell_type":"code","source":"# Transforms\nimage_size = 224\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(image_size),\n    transforms.RandomHorizontalFlip(),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:14:29.951527Z","iopub.execute_input":"2025-05-22T10:14:29.951833Z","iopub.status.idle":"2025-05-22T10:14:29.956564Z","shell.execute_reply.started":"2025-05-22T10:14:29.951809Z","shell.execute_reply":"2025-05-22T10:14:29.955896Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_transfer_model(\n    train_dir,\n    test_dir,\n    train_transform,\n    test_transform,\n    image_size=224,\n    batch_size=32,\n    epochs=30,\n    patience=3,\n    base_model_trainable=False,\n    optimizer_name=\"adam\",\n    loss_fn_name=\"cross_entropy\",\n    steps_per_epoch=150,\n    validation_split=0.2\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Dataset and split\n    full_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n    num_val = int(validation_split * len(full_dataset))\n    num_train = len(full_dataset) - num_val\n    train_dataset, val_dataset = random_split(full_dataset, [num_train, num_val])\n    val_dataset.dataset.transform = test_transform  # Use test transforms for validation\n\n    test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    class_names = full_dataset.classes\n    num_classes = len(class_names)\n\n    # Load base model\n    base_model = models.mobilenet_v2(pretrained=True)\n    base_model.classifier = nn.Sequential(\n        nn.Dropout(0.2),\n        nn.Linear(base_model.last_channel, num_classes)\n    )\n    if not base_model_trainable:\n        for param in base_model.features.parameters():\n            param.requires_grad = False\n\n    base_model.to(device)\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(base_model.parameters()) if optimizer_name == \"adam\" else optim.SGD(base_model.parameters(), lr=0.01)\n\n    # Early stopping\n    best_loss = float(\"inf\")\n    patience_counter = 0\n\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n\n    for epoch in range(epochs):\n        base_model.train()\n        train_loss = 0.0\n        for i, (inputs, labels) in enumerate(train_loader):\n            if i >= steps_per_epoch:\n                break\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = base_model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        # Validation\n        base_model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = base_model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        avg_train_loss = train_loss / steps_per_epoch\n        avg_val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        history[\"train_loss\"].append(avg_train_loss)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_accuracy\"].append(val_acc)\n\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n            best_model_state = base_model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load best model\n    base_model.load_state_dict(best_model_state)\n\n    # Test\n    base_model.eval()\n    all_preds = []\n    all_labels = []\n    test_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = base_model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    test_loss /= len(test_loader)\n    test_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n\n    print(f\"\\n✅ Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n\n    return {\n        \"model\": base_model,\n        \"history\": history,\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_accuracy,\n        \"true_classes\": all_labels,\n        \"predicted_classes\": all_preds,\n        \"class_labels\": class_names\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:14:30.850603Z","iopub.execute_input":"2025-05-22T10:14:30.850900Z","iopub.status.idle":"2025-05-22T10:14:30.867556Z","shell.execute_reply.started":"2025-05-22T10:14:30.850877Z","shell.execute_reply":"2025-05-22T10:14:30.866690Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"result = train_transfer_model(\n    train_dir = train_dir,\n    test_dir = test_dir,\n    train_transform=train_transform,\n    test_transform=test_transform,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:14:34.745901Z","iopub.execute_input":"2025-05-22T10:14:34.746385Z","iopub.status.idle":"2025-05-22T10:25:18.870512Z","shell.execute_reply.started":"2025-05-22T10:14:34.746360Z","shell.execute_reply":"2025-05-22T10:25:18.869830Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 177MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Train Loss: 1.0568, Val Loss: 2.0997, Val Acc: 0.4077\nEpoch 2/30, Train Loss: 0.7155, Val Loss: 1.7244, Val Acc: 0.4828\nEpoch 3/30, Train Loss: 0.5833, Val Loss: 1.5987, Val Acc: 0.5193\nEpoch 4/30, Train Loss: 0.5055, Val Loss: 1.5151, Val Acc: 0.5322\nEpoch 5/30, Train Loss: 0.4521, Val Loss: 1.5294, Val Acc: 0.5107\nEpoch 6/30, Train Loss: 0.4132, Val Loss: 1.4680, Val Acc: 0.5172\nEpoch 7/30, Train Loss: 0.3833, Val Loss: 1.4424, Val Acc: 0.5279\nEpoch 8/30, Train Loss: 0.3490, Val Loss: 1.4566, Val Acc: 0.5429\nEpoch 9/30, Train Loss: 0.3290, Val Loss: 1.4181, Val Acc: 0.5579\nEpoch 10/30, Train Loss: 0.3211, Val Loss: 1.4192, Val Acc: 0.5730\nEpoch 11/30, Train Loss: 0.3012, Val Loss: 1.4250, Val Acc: 0.5322\nEpoch 12/30, Train Loss: 0.2897, Val Loss: 1.4292, Val Acc: 0.5558\nEarly stopping triggered.\n\n✅ Test Loss: 1.4671, Test Accuracy: 0.5000\n\nClassification Report:\n                            precision    recall  f1-score   support\n\n           Apple Scab Leaf       0.50      0.60      0.55        10\n                Apple leaf       0.26      0.56      0.36         9\n           Apple rust leaf       0.60      0.60      0.60        10\n          Bell_pepper leaf       0.50      0.50      0.50         8\n     Bell_pepper leaf spot       0.50      0.11      0.18         9\n            Blueberry leaf       0.36      0.36      0.36        11\n               Cherry leaf       0.50      0.10      0.17        10\n       Corn Gray leaf spot       0.12      0.25      0.17         4\n          Corn leaf blight       0.57      0.33      0.42        12\n            Corn rust leaf       0.82      0.90      0.86        10\n                Peach leaf       0.60      0.33      0.43         9\n  Potato leaf early blight       0.36      0.62      0.45         8\n   Potato leaf late blight       0.18      0.25      0.21         8\n            Raspberry leaf       0.50      1.00      0.67         7\n             Soyabean leaf       0.62      0.62      0.62         8\nSquash Powdery mildew leaf       0.80      0.67      0.73         6\n           Strawberry leaf       1.00      1.00      1.00         8\n  Tomato Early blight leaf       0.62      0.56      0.59         9\n Tomato Septoria leaf spot       0.55      0.55      0.55        11\n               Tomato leaf       1.00      0.12      0.22         8\nTomato leaf bacterial spot       0.13      0.22      0.17         9\n   Tomato leaf late blight       0.36      0.50      0.42        10\n  Tomato leaf mosaic virus       0.50      0.30      0.37        10\n  Tomato leaf yellow virus       0.80      0.67      0.73         6\n          Tomato mold leaf       0.60      0.50      0.55         6\n                grape leaf       0.90      0.75      0.82        12\n      grape leaf black rot       0.83      0.62      0.71         8\n\n                  accuracy                           0.50       236\n                 macro avg       0.56      0.50      0.50       236\n              weighted avg       0.56      0.50      0.50       236\n\n","output_type":"stream"}],"execution_count":9}]}